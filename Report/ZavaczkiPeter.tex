\documentclass[a4paper,10pt]{report}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}
\definecolor{light-gray}{gray}{0.85}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Title Page
\title{
Handwritten symbol recognition using Scikit-learn
}
\author{Peter-Tibor Zavaczki}
\date{march 7, 2018}

\begin{document}
\maketitle

 
\chapter{About Scikit-learn}

 \section{Tool Purpose}
 %State here in one paragraph, in Natural Language (plain English), what kind of problem your system can solve.
 Scikit-learn is a machine learning library for Python, which features various classification, regression and clustering algorithms, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.
 
 \section{Installing scikit-learn}
 Prior to using scikit-learn, Python ($>$= 2.7 or $>$= 3.3) has to be installed along with the NumPy ($>$= 1.8.2) and SciPy ($>$= 0.13.3) libraries.

 \subsection{Installing steps}
 Some versions of Ubuntu come installed with Python 2.7.12 and Python 3.5.2, so for this installation we will consider that and install scikit-learn for Python 3.5.2.
 To ease installing packages for Python we will use pip. To install pip, we need the command \textit{sudo apt install python3-pip}. Please note that we have a 3 after python to signal that we will install pip for Python 3.x.
 After the previous step we install NumPy and SciPy by using the command \textit{sudo pip3 install numpy scipy}. This will download the libraries' latest version and automatically install them.
 As a final step, we use the command \textit{sudo pip3 install scikit-learn} to install scikit-learn.

 \section{Studied example}
 %Describe in one paragraph the example you have studied. Then detail the implementation in the next subsections.
 The studied example is \textbf{Recognizing hand-written digits} by \textbf{Gael Varoquaux}, a handwritten digit classificator by machine learning. It can recognize the 0-9 handwritten digits and convert them to digital characters.
 
 \subsection{How to run the example(s)}
 %Give step-by-step instructions on how to run the example(s) you have studied.   
 %steps for running the example.
 To run the given example, you need to have matplotlib, installed with \textit{sudo pip3 install matplotlib} and python3-tk, installed with \textit{sudo apt-get install python3-tk}. Then just use the command $python3\ ./plot\_digits\_classification.py$ from the folder of origin to run the example.
 
 \subsection{Algorithm}
 The given example relies on a few libraries which it imports and works with. These are matplotlib.pyplot, and from sklearn, the datasets, svm, metrics libraries. After the libraries have been loaded, the application loads the processed dataset using the datasets.load\_digits() command. 
 
 The images and the targets of the digits dataset is zipped into touples and added to a lists, so that it can be worked with in the following nested for-each loop (first level iterating by index, second level iterating by (image, prediction) touple). Please note that the loop only takes the first 4 (image, target) touples! In the mentioned for loop, at each iteration a new subplot is activated, the axis' are turned off (we are displaying an image and labelling it to see what it is, we are not actually displaying an actual plot), an image is shown on the axis with the gray\_r colormap set and the interpolation set to nearest (this is the best choice when a small image is enlarged), then a title is set for each sepparate subplot, which signals the character trained using that data sample.
 
 In the following step the number of image samples is saved in the n\_samples variable. In the data variable a reshaped version of the digits' images' array is stored in the (samples, feature) matrix format. A Support Vector Classifier is instantiated with a gamma of value 0.001 and then the first half of the dataset is used for training.
 
 After training with the first half of the dataset, the second half's targets are stored in the 'expected' variable and the predictions from the second half of the dataset are stored in the 'predicted' variable.
 
 After predicting the second half of the dataset, we print the SVC's prameters, which in this case is all default, except for gamma and the classification report, which consists of listing the possible cases and the precision, recall and f1-scores calculated for them, along the number of samples of that case in the predicted dataset. The second part of printing the prediction data is the confusion matrix, which represents the expected values on the rows and the predicted values on the columns.
 
 In the next step, the second half of the dataset and the predictions are zipped into a list of touples, so that the first four predicted images can be displayed similarly to the first four training images before.
 
 As a last step, the plot is shown so that we can see the results.
 
 
\chapter{Proposed project}
 The proposed project is a handwritten letter recognizer, working on the \href{https://www.kaggle.com/ashishguptajiit/handwritten-az/}{''EMINST handwritten letters dataset, from kaggle''}.
 
 \section{The dataset's format}
 The above mentioned dataset is split into two csv files, representing a training set of 171.610.185 bytes of data and a testing set of 28.625.756 bytes.
 Each row of the csv has 785 columns, which represents an instance of data.
 On the first column, the number represents the position of the letter in the english alphabet (1 for a, 26 for z).
 The rest of the row's columns represent the values of a flattened 28x28 pixel grayscale image (784 values on a row, each representing a pixel), with values ranging from 0 to 255.
 
 \section{The code}
 The code is written in Python, as such the lines starting with a hash symbol (\#) are considered comments.\\
 The implementation boils down to a few little snippets regarding the manipulation of the dataset, creating the classifier, fitting the training data, then predicting the testing data, then outputting the classification report and confusion matrix generated by the classifier.
 
 \subsection{Importing the necessary libraries}
 In this section we import the libraries, with which we will work later on. Namely: numpy, svm and metrics from sklearn and joblib from sklearn.externals.\\
 \code{\# Import numpy / pandas for csv loading}\\
 \code{import numpy as np}\\
 \code{}\\
 \code{\# Import classifiers and performance metrics}\\
 \code{from sklearn import svm, metrics}\\
 \code{}\\
 \code{\# Import pandas or joblib for storing data}\\
 \code{\# import pandas as pd}\\
 \code{from sklearn.externals import joblib}\\

 \subsection{Manipulating the training dataset} 
 In this section we load the raw training dataset using numpy, then we split it by the first column and the rest (the letter and its representation). After we have split the data into the specific parts, we flatten the letter representations into a single line, so that, in case it is represented as an nxn matrix, it will be a vector of length nxn.\\
 \code{\# The letters dataset}\\
 \code{lettersTrainRaw = np.loadtxt(fname = "./emnist-letters-train.csv", delimiter = ',')}\\
 \code{lettersTrainTarget = lettersTrainRaw[:, 0]}\\
 \code{lettersTrainData = lettersTrainRaw[:, 1:]}\\
 \code{}\\
 \code{\# To apply a classifier on this data, we need to flatten the image,}\\
 \code{\# to turn the data in a (samples, feature) matrix:}\\
 \code{n\_samplesTrain = len(lettersTrainData)}\\
 \code{dataTrain = lettersTrainData.reshape((n\_samplesTrain, -1))}\\

 \subsection{The classifier}
 In this step we create a SVC (Support Vector Classifier) to which we will fit our training data and then predict the testing data.\\
 TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\\
 TALK ABOUT THE CLASSIFIER\\
 TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO\\
 \code{\# Create a classifier: a support vector classifier}\\
 \code{classifier = svm.SVC(gamma=0.001)}\\
 \code{\# We learn the letters in the training dataset}\\
 \code{classifier.fit(dataTrain[:n\_samplesTrain // 1], lettersTrainTarget[:n\_samplesTrain // 1])}\\
 
 \subsection{Manipulating the testing dataset}
 In this secton we modify the loaded testing dataset similarly to how we did with the training dataset.\\
 
 \code{\# Now predict the value of the letters in the tresting set}\\
 \code{lettersTestRaw = np.loadtxt(fname = "./emnist-letters-train.csv", delimiter = ',')}\\
 \code{lettersTestTarget = lettersTestRaw[:, 0]}\\
 \code{lettersTestData = lettersTestRaw[:, 1:]}\\
 \code{}\\
 \code{n\_samplesTest = len(lettersTestData)}\\
 \code{dataTest = lettersTestData.reshape((n\_samplesTest, -1))}\\

 \subsection{Predicting the testing dataset and showing the results}
 In this section we take the already trained classifier and use it to predict a different dataset, called the testing dataset. After the prediction we get some results, such as a report, containing the precision, recall, f1-score and the support of each predicted value. After the report has been printed, we print a confusion matrix which shows each value and what has been predicted.\\
 \code{predicted = classifier.predict(dataTest)}\\
 \code{print("Classification report for classifier \%s:\textbackslash n\%s \textbackslash n"}\\
 \code{\% (classifier, metrics.classification\_report(lettersTestTarget, predicted)))}\\
 \code{print("Confusion matrix:\textbackslash n\%s" \% metrics.confusion\_matrix(lettersTestTarget, predicted))}\\
 
 \section{Results}
 The results are composed of a report, containing the precision, recall, f1-score and the support of each predicted value and a confusion matrix. For the given dataset, the result looks as can be seen in the result.txt file.\\
 
 
 
\end{document}
